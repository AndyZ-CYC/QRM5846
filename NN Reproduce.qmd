---
title: "Neural-Network Model"
author: "Jiachen Liu, Yaoyuan Zhang"
format: pdf
editor: visual
---

```{r setup, include=FALSE, echo=FALSE}

```

# RNN-LSTM Model

## Recurrent Neural Network(RNN)

A **recurrent neural network** is a class of artificial neural network that uses sequential or time series data. Unlike Feedforward Neural Network, RNN allows the output from some nodes to affect subsequent input to the same nodes by using connections between nodes to create cycles. As a result, the hidden layers produce the outputs with the input information and prior "memory" received from previous learning.

![Rolled RNN and Unrolled RNN](images/unroll-RNN.jpg)

Another distinguish characteristic of RNN is that they share parameters across each layer of the network. Unlike feedforward neural networks having individual weight and bias for each node in one layer, recurrent neural networks share the same weight parameter within each layer. However, these weights are still adjusted during the processes of backpropagation and gradient descent to facilitate reinforcement learning.

In feedforward neural network, backpropagation algorithm was used to calculate the gradient with respect to the weights. Recurrent neural network, on the other side, leverage backpropagation through time (BPTT) algorithm to determine the gradient as BPTT is specific to sequential data.

## Activation Functions

In neural networks, an activation function determines whether a neuron should be activated and typically maps the input to $[0, 1]$ or $[-1, 1]$. The followings are two of the most commonly used activation functions and will be adopted later:

#### Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

#### Tanh (Hyperbolic tangent)

$$
tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$

## Long Short-term Memory (LSTM)

Long short-term memory network, usually known as LSTM, is a specific RNN architecture first introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. Recall with an RNN, similar with human reading a book and remembering what happened in the earlier chapter, it remembers the previous information and use it for processing the current input. The shortcoming of the NN is that it is not able to remember long term dependencies due to the vanishing gradient. The LSTM is designed to alleviate and avoid such issues.

The LSTM consists of three parts:

-   **Forget Gate**: Choose whether the information coming from the previous time stamp should be remembered or can be forgotten
-   **Input Gate**: Learn new information from the input to this cell
-   **Output Gate**: Passes the updated information tot the next time stamp

### Forget Gate

In an LSTM cell, the cell first need to decide if the information from previous time stamp should be kept or forgotten. The equation of the forget gate is:

$$
f_t = \sigma(W_f\cdot[x_t, h_{t-1}] + b_f)
$$

Where

-   $x_t = \text{input to the current time stamp}$
-   $h_{t-1} = \text{hidden state of the previous time stamp}$
-   $W_f = \text{weight matrix associated with hidden state}$
-   $b_f = \text{constant}$

After that, a `sigmoid` function is applied over $f_t$ and make it a number between $0$ and $1$. Then $f_t$ is multiplied with the previous cell state. If $f_t=0$, it means the network will forget everything from the previous time stamp while $f_t=1$ represents that the network will remember everything.

### Input Gate and new information

Next we decide what new information we will store in the cell state. First, the input gate decides which values we'll update with `sigmoid` activation function:

$$
i_t = \sigma(W_i\cdot[x_t, h_{t-1}] + b_i)
$$

Where

-   $W_f = \text{weight matrix of the input associated with hidden state}$

Next, the new information is sent through a `tanh` layer to create the new candidate values:

$$
\tilde{C_t} = tanh(W_C\cdot[x_t, h_{t-1}] + b_C)
$$

### New Cell State $C_t$

With previous work, the LSTM cell now updates the new cell state for the current time stamp as:

$$
C_t = f_t * C_{t-1} + i_t * \tilde{C_t}
$$

The current cell state $C_t$ combines how much we decide to remember from the previous cell state $C_{t-1}$ scaled by the forget gate and how much we wish to take in from the new current input $\tilde{C_t}$ scaled by the input gate.

### Output Gate

Finally, the cell needs to decide what it is going to output and by how much. The filter of the output is the output gate, with the following equation:

$$
o_t = \sigma(W_o\cdot[x_t, h_{t-1}] + b_o)
$$

The equation of the output gate is very similar with the forget gate and the input gate. Then, we push the cell state $C_t$ through the `tanh` activation function to maintain the value staying in between $-1$ and $1$, and multiply it by the output gate:

$$
h_t = o_t * tanh(C_t)
$$

### Overall Module

The previous steps conclude the architecture of the LSTM. The whole process can be summarized and displayed as the following:

![LSTM Chain](images%5CLSTM-chain.png)

## Application in Tensorflow

We implement the LSTM by using the package `tensorflow`. The LSTM model in the `keras` package of `tensorflow` follows our previous description by using the `tanh` function as the activation function and the `sigmoid` function as the recurrent activation function.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#| warning: false
library(tidyquant)
library(tidyverse)
library(tensorflow)
library(keras)
# install_keras()
# install_tensorflow(version = "nightly")
```

```{r}
start_date = "2013-01-01"
end_date = "2017-12-31"

sp_prices <- tq_get("NFLX", #"^GSPC",
                 get = "stock.prices",
                 from = start_date,
                 to = end_date)

stock <- sp_prices |>
  arrange(date) |>
  select(date, adjusted) |>
  column_to_rownames('date')
```

```{r}
train_set = stock[1:1008,]

# Max Min Scaler
max_min_scale <- function(x, name = 'value') {
  df <- data.frame((x- min(x)) /(max(x)-min(x)))
  colnames(df) <- name
  return(df)
}

max_min_scale_reverse <- function(y, x) {
  min(x) + y * (max(x)-min(x))
}

train_scaled <- train_set %>% max_min_scale
```

```{r}
# data preparation
data_prep <- function(scaled_data, prediction = 1, lag = 22){
  x_data <- t(sapply(1:(dim(scaled_data)[1] - lag - prediction + 1),
                  function(x) scaled_data[x: (x + lag - 1), 1]))
  x_arr <- array(data = as.numeric(unlist(x_data)), 
                 dim = c(nrow(x_data), lag, 1))
  y_data <- t(sapply((1 + lag):(dim(scaled_data)[1] - prediction + 1), 
                     function(x) scaled_data[x: (x + prediction - 1), 1])) 
  y_arr <- array(data = as.numeric(unlist(y_data)), 
                 dim = c(length(y_data), prediction, 1))
  return(list(x = x_arr, y = y_arr))
}

x_train = data_prep(train_scaled)$x
y_train = data_prep(train_scaled)$y

dim(x_train)
dim(y_train)
```

```{r}
num_neurons = 50
learning_rate = 0.002
#num_train_iterations = 4000

# Define LSTM
get_model <- function(){
  model <- keras_model_sequential() |>
    layer_lstm(units = num_neurons, 
               batch_input_shape = c(1, 22, 1), 
               activation = 'relu') |>
    layer_dense(units = 1)
  model %>% compile(loss = 'mse', 
                       optimizer = optimizer_adam(learning_rate = learning_rate), 
                       metrics = 'mae')
}


lstm_model <- get_model()
summary(lstm_model)
```

```{r}
lstm_model %>% fit(
    x = x_train,
    y = y_train,
    batch_size = 1,
    epochs = 10,
    verbose = 2,
    #callbacks = Callbacks,
    shuffle = FALSE
) -> history

plot(history)
```

```{r}
predictions <- vector(length = 251L)
pred_data <- train_set

for (i in 1:251){
  n <- 1008 + i
  test_set = pred_data[(n-22):n]
  test_scaled <- test_set %>% max_min_scale
  x_test = data_prep(test_scaled)$x
  y_pred_scaled <- predict(lstm_model, x_test, verbose = 2)
  y_pred <- max_min_scale_reverse(y_pred_scaled, test_set)
  predictions[i] <- y_pred
  pred_data <- pred_data %>% append(y_pred)
}

plot(predictions,type = 'l')
```

```{r}
predictions <- vector(length = 251L)

for (i in 1:251){
  n <- 1008 + i
  test_set = stock$adjusted[(n-22):n]
  test_scaled <- test_set %>% max_min_scale
  x_test = data_prep(test_scaled)$x
  y_pred_scaled <- predict(lstm_model, x_test, verbose = 0)
  y_pred <- max_min_scale_reverse(y_pred_scaled, test_set)
  predictions[i] <- y_pred
}

plot(predictions,type = 'l', col = 'red')
lines(stock$adjusted[(1259-250):1259])
```

```{r}
pred_df <- data.frame('pred' = predictions,
                      'actual' = stock$adjusted[(1259-250):1259]) %>% 
  pivot_longer(cols = c(pred, actual), names_to = "type", 
               values_to = "price") %>% 
    mutate(fill = case_when(type == 'pred' ~ "dark blue", T ~ 'violet'))

ggplot(pred_df, aes(x=price, fill=fill, col = 'gray')) +
  geom_histogram(binwidth=5, colour="black", position="dodge") +
 scale_fill_identity()
```

```{r}
# evaluation
model_eval <- function(y, pred){
  mse <- mean((pred - y)^2)
  mae <- mean(abs(pred-y))
  rmse <- sqrt(mean(pred-y)^2)
  rsq <- sum((pred-mean(y))^2)/sum((y-mean(y))^2)
  return(c(mse, mae, rmse, rsq))
}

model_eval(stock$adjusted[(1259-250):1259], predictions)
```

## Application in Tensorflow

```{r}
# normalization
prices[, -1] <- prices[, -1] %>% apply(2, scale)

# train test split
split_date <- "2019-05-17"
prices |> filter(date < split_date) %>% as.matrix() -> train_data
prices |> filter(date >= split_date) %>% as.matrix() -> test_data
```

```{r}
# data preparation
data_prep <- function(scaled_data, prediction = 1, lag = 22, y_type = 2){

  x_data <- t(sapply(1:(dim(scaled_data)[1] - lag - prediction + 1),
                  function(x) scaled_data[x: (x + lag - 1), -1]))
              
  x_arr <- array(data = as.numeric(unlist(x_data)), 
                 dim = c(nrow(x_data), lag, 1))
  
  # the 2nd column is opening price, 7th is adjusted price
  y_data <- t(sapply((1 + lag):(dim(scaled_data)[1] - prediction + 1), 
                     function(x) scaled_data[x: (x + prediction - 1), y_type])) 
  
  y_arr <- array(data = as.numeric(unlist(y_data)), 
                 dim = c(length(y_data), prediction, 1))
  
  return(list(x = x_arr, y = y_arr))
}

x_train = data_prep(train_data, 2)$x
y_train = data_prep(train_data, 2)$y
x_test = data_prep(test_data, 2)$x
y_test = data_prep(test_data, 2)$y
```

```{r}
# LSTM model
lstm_model <- keras_model_sequential()

lstm_model |>
  layer_lstm(units = 10, 
             batch_input_shape = c(7, 22, 1), 
             return_sequences = TRUE, 
             stateful = TRUE) |>
  layer_flatten() |>
  layer_dense(units = 1)
  

lstm_model %>% compile(loss = 'mse', 
                       optimizer = "adam", 
                       metrics = 'mae')
summary(lstm_model)
```

```{r}
lstm_model %>% fit(
    x = x_train,
    y = y_train,
    batch_size = 1,
    epochs = 10,
    verbose = 0,
    shuffle = FALSE
) -> history

plot(history) #因为output不止1个算不出loss所以不会converge
```

```{r}
# evaluation
model_eval <- function(y, pred){
  mse <- mean((pred - y)^2)
  mae <- mean(abs(pred-y))
  rmse <- sqrt(mean(pred-y)^2)
  rsq <- sum((pred-mean(y))^2)/sum((y-mean(y))^2)
  return(c(mse, mae, rmse, rsq))
}

model_eval(y_test, keras::predict(lstm_model, x_test))
```
