---
title: "Presentation deck_NN test"
editor: visual
format: 
  beamer:
    navigation: horizontal
    theme: CambridgeUS
#    theme: Montpellier
    colortheme: spruce
#    colortheme: lily
    #toc: true
#    theme: ../slides.scss
    slide-number: true
    chalkboard: 
      boardmarker-width: 5
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    editor: source
---

# Reproduce - Understand Article

![Design Overview of RNN Model](Images/RNN_reproduce_structure.png)

# Reproduce - Understand Article

## Data: Stock price of five companies from "2013-01-01" to "2017-12-31"

> "In particular, a total of 1259 days of data was collected. The first 1008 data points are for the first 4 years (2013 to 2016) and the last 251 data are for each day in 2017."

\vspace{0.5cm}

## Data Process: Scaling: Min-Max Scale

$$
x' = \frac{x - min(x)}{max(x) - min(x)}
$$ We will need to rescale data back after predicting.

# Reproduce - Understand Article

## Parameter

![](Images/RNN_parameter.png)

## LSTM Cell

![](Images/RNN_lstm_cell.png)

## Loss

![](Images/RNN_loss.png)

# Reproduce - Understand Article

![The Neural Network Loop](Images/RNN_loop.png)

# Reproduce - Data

```{r, echo=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyquant)
library(tidyverse)
library(tensorflow)
library(keras)
```

```{r, echo=TRUE}
start_date = "2013-01-01"; end_date = "2017-12-31"

nflx_prices <- tq_get("NFLX", get = "stock.prices",
                 from = start_date, to = end_date)

stock <- nflx_prices |> arrange(date) |>
  select(date, adjusted) |> column_to_rownames('date')

head(stock, 5)
```

# Reproduce - Train-Test Split and Scale

```{r, echo=TRUE}
# Max Min Scale
max_min_scale <- function(x, name = 'value') {
  df <- data.frame((x- min(x)) /(max(x)-min(x)))
  colnames(df) <- name; df}
max_min_scale_reverse <- function(y, x) {
  min(x) + y * (max(x)-min(x))}

train_set <- stock[1:1008,]
test_set <- stock[1009:nrow(stock),]
train_scaled <- train_set %>% max_min_scale
head(train_scaled, 3)
```

# Reproduce - Data Preperation for Fitting

```{r, echo=TRUE}
data_prep <- function(scaled_data, prediction = 1, lag = 22){
  x_data <- t(sapply(1:(dim(scaled_data)[1] - lag - prediction + 1),
                  function(x) scaled_data[x: (x + lag - 1), 1]))
  x_arr <- array(data = as.numeric(unlist(x_data)), 
                 dim = c(nrow(x_data), lag, 1))
  y_data <- t(sapply((1 + lag):(dim(scaled_data)[1] - prediction + 1), 
                     function(x) scaled_data[x: (x + prediction - 1), 1])) 
  y_arr <- array(data = as.numeric(unlist(y_data)), 
                 dim = c(length(y_data), prediction, 1))
  return(list(x = x_arr, y = y_arr))}

x_train = data_prep(train_scaled)$x
y_train = data_prep(train_scaled)$y
cat('x_dim: (', dim(x_train), ') || y_dim: (', dim(y_train), ')')
```

```{r}
cat('x dim:', dim(x_train))
```

# Reproduce - LSTM Setup

```{r, echo=TRUE}
num_neurons = 50
learning_rate = 0.002
# Define LSTM
get_model <- function(){
  model <- keras_model_sequential() |>
    layer_lstm(units = num_neurons, 
               batch_input_shape = c(1, 22, 1), 
               activation = 'relu', 
               stateful = TRUE) |>
    layer_dense(units = 1) 
  
  model %>% compile(loss = 'mse', metrics = 'mae',
    optimizer=optimizer_adam(learning_rate=learning_rate))
}
```

# Reproduce - LSTM Structure

```{r, echo=TRUE}
lstm_model <- get_model()
summary(lstm_model)
```

# Reproduce - LSTM Fitting

```{r, echo=TRUE}
set_random_seed(1209)
lstm_model <- get_model()
lstm_model %>% fit(x = x_train, y = y_train, batch_size = 1,
    epochs = 15, verbose = 2, shuffle = FALSE) -> history
```

```{r, fig.width=6, fig.height=2}
data.frame("x" = seq(1, 15), "y" = history$metrics$loss) -> df

df %>% ggplot(aes(x = x, y = y)) +
    geom_line(col = '#42a5f5') +
  geom_point(col = '#2c8fdf') + 
  labs(title = 'Loss History',
       x = 'Epoch', y = 'Loss(mse)')
```

# Reproduce - LSTM Result

```{r, echo=TRUE}
model_prediction <- function(test_len, whole_data, model){
  t <- test_len # test size
  Tt <- nrow(whole_data) # whole data size
  t_start <- Tt - t + 1 # test start date
  predictions <- vector(length = t)
  for (i in 1:t){
    n <- t_start - 1 + i
    test_set = pull(whole_data)[(n-22):n]
    test_scaled <- test_set %>% max_min_scale
    x_test = data_prep(test_scaled)$x
    y_pred_scaled <- predict(model, x_test, verbose = 0)
    y_pred <- max_min_scale_reverse(y_pred_scaled, test_set)
    predictions[i] <- y_pred
  }
  pred_df <- data.frame('date' = rownames(whole_data)[t_start:Tt] %>% as_date(),
    'pred' = predictions, 'actual' = pull(whole_data)[t_start:Tt])
  pred_df
}
```

# Reproduce - LSTM Result

```{r, echo=TRUE}
model_prediction(length(test_set), stock, lstm_model) -> pred_df
pred_df %>% head(2)
```

```{r, fig.width=4, fig.height=2}
pred_df_long <- pred_df %>% pivot_longer(cols = c(pred, actual), 
                       names_to = "type", values_to = "price")

pred_df_long %>% ggplot(aes(x = date, y = price, color = type)) +
  geom_line() +
  labs(x = 'Date', y = 'Price($)', color = 'Type') +
  theme(legend.position = 'top')
```

# Result Comparison - Article Result

![Comparison of NFLX Monthly Mean](images/RNN_monthly_result.png)

# Result Comparison - Our Result

```{r, fig.width=5, fig.height=3.5}
pred_df_monthly <- pred_df_long %>% mutate(month = month.abb[month(date)]) %>%
  group_by(month, type) %>% summarise(avg_price = mean(price), .groups = 'drop')

pred_df_monthly$month <- factor(pred_df_monthly$month, levels = month.abb)

ggplot(pred_df_monthly, aes(x=month, y=avg_price, fill=type)) +
  geom_bar(stat = "identity", position="dodge", width = 0.65) +
  scale_fill_manual(values = c("actual" = '#393b79', 'pred' = '#de9ed6')) +
  labs(title = 'Comparison of NFLX Monthly Mean, 2017',
       y = 'Adjusted Price', x = 'Date', fill = '') +
  theme_classic() +
  theme(legend.position = 'bottom') 
```

# Result Comparison - \|% of Error\|

```{r}
data.frame('mean'= 2.00, 'std'= 1.54, 'variabce' = 2.38,
           'min' = 0.00, '25_percentile' = 0.80, '50_percentile' = 1.65, 
           '75_percentile' = 2.96, 'max' = 9.25, 'R_squared' = 0.9589) %>% 
  pivot_longer(cols = c(mean:R_squared), names_to = '|% of Error|', 
               values_to = 'Article') -> comparison

pred = pred_df$pred
y = pred_df$actual
rsq <- sum((pred_df$pred-mean(y))^2)/sum((y-mean(y))^2)

per_err <- pred_df %>% mutate(percent_err = abs(pred-actual)/actual * 100) %>% 
  pull(percent_err)
```

```{r,echo=TRUE}
comparison %>% cbind('Our Result' = c(mean(per_err), StdDev(per_err),
                      StdDev(per_err)^2, quantile(per_err), rsq))
```

# Model Improving - get S&P 500, set val

```{r, echo=TRUE}
sp_prices <- tq_get("^GSPC", get = "stock.prices",
                 from = "2000-01-01", to = "2021-12-31")
return <- sp_prices |> arrange(date) |>
  mutate(return = log(adjusted) - log(lag(adjusted))) |>
  drop_na(return) %>% select(date, return)
# scale
return$return <- max_min_scale(return$return) %>% pull(value)
# train test split
val_date <- "2019-01-01"; test_date <- '2020-01-01'
train_data_n <- return |> filter(date <= val_date) %>% column_to_rownames('date')
val_data_n <- return |> filter(date < test_date & date > val_date) %>% column_to_rownames('date')
test_data_n <- return |> filter(date >= test_date) %>% column_to_rownames('date')
# data preparation
x_train_new = data_prep(train_data_n)$x; y_train_new = data_prep(train_data_n)$y
x_val_new = data_prep(val_data_n)$x; y_val_new = data_prep(val_data_n)$y
```

```{r}
cat('x_tr_dim: (', dim(x_train_new), ') || y_tr_dim: (', dim(y_train_new), ')\n')
cat('x_val_dim: (', dim(x_val_new), ') || y_val_dim: (', dim(y_val_new), ')')
```

# Model Improving - set new model

```{r, echo=TRUE}
learning_rate = 0.0001
# Define LSTM
get_model_new <- function(){
  model <- keras_model_sequential() |>
    layer_lstm(units = 50, batch_input_shape = c(1, 22, 1), 
               return_sequences = TRUE, stateful = TRUE) |>
    layer_dropout(rate = 0.5) |>
    layer_lstm(units = 50) |>
    layer_dropout(rate = 0.5) |>
    layer_dense(units = 1)
  
  model %>% compile(loss = 'mse', metrics = 'mae',
    optimizer=optimizer_adam(learning_rate=learning_rate))
}
```

# Model Improving - set new model

```{r, echo=TRUE}
lstm_model_new <- get_model_new()
summary(lstm_model_new)
```

```{r}
validation_data <- list(x_val_new, y_val_new)
set_random_seed(1209)
lstm_model_new <- get_model_new()
lstm_model_new %>% fit(x = x_train_new, y = y_train_new, batch_size = 1,
    epochs = 15, verbose = 2, shuffle = FALSE, 
    validation_data = validation_data) -> history_new
```

```{r}
set_random_seed(1209)
lstm_model2 <- get_model()
lstm_model2 %>% fit(x = x_train_new, y = y_train_new, batch_size = 1,
    epochs = 15, verbose = 2, shuffle = FALSE, 
    validation_data = validation_data) -> history2
```

```{r}
model_prediction(length(test_data_n), stock, lstm_model) -> pred_df

model_eval <- function(y, pred){
  mse <- mean((pred - y)^2)
  mae <- mean(abs(pred-y))
  rmse <- sqrt(mean(pred-y)^2)
  rsq <- sum((pred-mean(y))^2)/sum((y-mean(y))^2)
  return(c(mse, mae, rmse, rsq))
}

model_eval(y_test, predict(lstm_model_new, x_test))
```

```{r}

```

# Model Comparison - ARIMA and LSTM

# Thank Your

Any Questions?
