---
title: "Performances of the Moving Average Model and the Artificial Neural Network on the Forecast of Stock Market Indices"
author: "Peilin Jing, Jiachen Liu, Yaoyuan Zhang, Rashi Lodhi"
editor: visual
format: 
  beamer:
    navigation: horizontal
    theme: CambridgeUS
#    theme: Montpellier
    colortheme: spruce
#    colortheme: lily
    toc: true
#    theme: ../slides.scss
    slide-number: true
    chalkboard: 
      boardmarker-width: 5
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    editor: source
---

```{r setup, include=FALSE, echo=FALSE}

```

# Introduction

## Abstract

This project delves into the theoretical framework of two types of models. It aims to compare and contrast the performances of the moving average model and the artificial neural network on the prediction of stock market indices. We used the data collected from Yahoo Finance with daily frequency for the period from 1 January 2000 to 31 December 2021. By using a rolling window approach, we evaluated ARIMA-SGARCH to reflect the specific time-series characteristics and have better predictive power than the simple ARIMA model and Recurrent Neural Network models. In order to assess the precision and quality of these models in forecasting, we compared their equity lines, their forecasting error metrics and their performance metrics. The main contribution of this research is to show that the hybrid ARIMA SGARCH model outperforms the other models over the long term.

## Importance of this topic

The importance of this topic can be condensed to 4 points:

-   **Interest in the area**: attracted attention of researchers, investors, speculators, and governments
-   **ARIMA hybrid over ARIMA**: financial time series often do not follow ARIMA assumptions
-   **newest ML techniques to improve models**: We use Recurrent Neural Network(RNN) model to determine if it can reflect the specific time series characteristics and predict better.

# Moving Average Model Methodology and Data

## Data Analysis

The first step in the process was cleaning the data. Then, we transformed the adjusted price into a daily logarithmic return, which was calculated according to the following formula: 
$$
r_t = ln\frac{P_t}{P_{t-1}}
$$ 
Reasons to choose log returns: - can be added across time periods to create cumulative returns - easy to convert between log return and simple return - log return follows normal distribution

Advantages to log return having normal distribution: - Distribution only dependent on mean and sd of sample - forecast with higher accuracy (log return) - Stock prices cannot be normal distribution

## Descriptive Statistics - Stock prices

This presents the descriptive statistics of the adjusted closing prices

```{r, out.width = "200px"}
knitr::include_graphics("images/Descriptive_Statistics.png")
```

## Descriptive Statistics - Stock prices II

There are a few periods, such as 2008, 2011, 2015, and 2018, that show high volatility of returns. We can expect to build more accurate forecasting models if we are able to mitigate and "smooth" such periods.

```{r, out.width = "200px"}
knitr::include_graphics("images/PricesPlot.png")
```

Stock prices of SP 500 are not normally distributed!

## Descriptive Statistics - Log Returns

This presents the descriptive statistics of the adjusted closing prices

```{r, out.width = "200px"}
knitr::include_graphics("images/LogReturnDescriptive.png")
```

We use log returns to build models!

## Methodology - ARIMA (p,d,q)

The ARMA process is the combination of the autoregressive model and moving average designed for a stationary time series.

Autoregression (AR) describes a stochastic process, and AR(p) can be denoted as shown below: $$
  y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \varepsilon_{t},
$$ where $\varepsilon_{t}$ is white noise. This is a multiple regression with lagged values of $y_{t}$ as predictors.

The moving average process of order q is denoted as MA(q) and the created time series contains a mean of q lagged white noise variables shifting along the series. $$
  y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},
$$ where $\varepsilon_{t}$ is white noise. This is a multiple regression with past errors as predictors.

## Methodology - ARIMA (p,d,q)

d is the number of differencing done to the series to achieve stationarity with I (d) so the ARIMA model can be expressed as $$\text{Expand:}\qquad
  y_t = c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1\varepsilon_{t-1} + \varepsilon_t
$$ p is the number of autoregressive terms (AR) d is the number of differencing (I) q is the number of moving average terms (MA)

## Methodology - ARCH(p)

The `ARCH(p)` model is given: $$\sigma_t^2 = \omega + \sum_{i=1}^{p}\alpha_{i}u^2_{t-i}$$

-   Most volatility models derive from this
-   Returns have a conditional distribution (here assumed to be normal)
-   ARCH is not a very good model and almost nobody uses it.
-   The reason is that it needs to use information from many days before $t$ to calculate volatility on day $t$. That is, it needs a lot of lags.
-   The solution is to write it as an ARMA model.
-   That is, add one component to the equation, $\beta\sigma_{t-1}$.

## Methodology - GARCH(r,s) and Hybrid ARIMA-SGARCH

The `GARCH(p,q)` model is $$\sigma_t^2 = \omega + \sum_{i=1}^{p}\alpha_{i}u^2_{t-i} + \sum_{j=1}^{q}\beta_{j}\sigma^2_{t-j}$$ Where: $\alpha$ is news. $\beta$ is memory. The size of $(\alpha + \beta)$ determines how quickly the predictability of the process dies out.

This leads us to lastly, ARIMA-SGarch

## ARIMA SGARCH - Overview

Stock prices can be tremendously volatile during economic growth as well as recessions. When homoskedasticity presumption is violated, it affects the validity or power of statistical tests when using ARIMA models. We consider the SGARCH effect. The error term of the ARIMA model in this process follows SGARCH(1,1) instead of being assumed constant like the ARIMA model.

## ARIMA SGARCH - Steps

1)  We conduct a rolling forecast based on an ARIMA-SGARCH model with window size(s) equal to 1000.

2)  The optimized combination of p and q which has the lowest AIC is used to predict return for the next point. At the end, the vector of forecasted values has the length of 3530 elements

3)  We describe and review our implementation of dynamic ARIMA(p,1,q)-SGARCH(1,1) models with GED distribution and window size(s) equal to 1000.

4)  we evaluate the results based on error metrics, performance metrics, and equity curves.

## Iteration of the forecasting model ARIMA(p,1,q)-SGARCH(1,1)

```{r, out.width = "300px"}
knitr::include_graphics("images/ARIMA_iteration.png")
```

## Flowchart of the forecasting model ARIMA(p,1,q)-SGARCH(1,1).

```{r, out.width = "250px"}
knitr::include_graphics("images/ARIMA_SGARCH_methodology.png")
```


# Moving Average Model - Coding

## Data import

```{r include=FALSE, echo=FALSE}
#Load packages
library(tidyquant)
library(fpp3)
library(rugarch)
library(rmgarch)
library(forecast)
```

```{r message = FALSE, echo=TRUE}
# Import Data
sp <- getSymbols(Symbols = "^GSPC", from = "2000-01-01", 
                 to = "2021-12-13", src = "yahoo", 
                 adjust=TRUE, auto.assign = FALSE)
sp_prices <- Ad(sp)
head(sp_prices)

```

## Return calculation
```{r echo=TRUE}
#Compute the log returns
returns <- CalculateReturns(sp_prices) %>% na.omit()
data <- returns
```
## Return calculation
```{r}
plot(returns)
```

## 'rugarch' package exploration
- **ugarchspec()**: Method for creating a univariate GARCH specification object prior to fitting.
- **ugarchfit()**: Method for fitting a variety of univariate GARCH models.
- **ugarchroll()**: Method for creating rolling density forecast from ARMA-GARCH models with option for refitting every n periods with parallel functionality.
- **ugarchboot()**: Method for forecasting the GARCH density based on a bootstrap procedures (see details and references).
- **ugarchforecast()**:Method for forecasting from a variety of univariate GARCH models.
- **ugarchfilter()**: Method for filtering a variety of univariate GARCH models.
- **ugarchpath()**: Method for simulating the path of a GARCH model from a variety of univariate GARCH models. 

## Specify sGarch model

```{r echo = TRUE}
# Specify sGARCH model
spec <- ugarchspec(
    variance.model =
      list(model = "sGARCH",
           garchOrder = c(1,1)),
      mean.model =
      list(armaOrder = c(0,0),
      include.mean = TRUE),
    distribution.model = "ged"
)
```

We choose the best model from the paper and reproduce it. The best model is hybrid model ARIMA(p,1,q)-SGARCH(1,1)
with GED distribution (SGARCH.GED 1000), so we define the model = "sGARCH" and define the distribution model as ged.

## Fit Arima-sGARCH Model

```{r echo = TRUE}
# Fit to the data
data -> y

sGARCH <- ugarchfit(spec = spec,
                     data = y,
                    solver = 'hybrid')
```
The solver parameter accepts a string stating which numerical optimizer to use to find the parameter estimates.The “hybrid” strategy solver first tries the “solnp” solver, in failing to converge then tries then “nlminb”, the “gosolnp” and finally the “nloptr” solvers. 
The out.sample option is provided in order to carry out forecast performance testing against actual data.

## Fit Arima-sGARCH Model

```{r}
sGARCH
```

## Fit Arima-sGARCH Model

```{R echo = TRUE}
# Results information criteria
infocriteria(sGARCH)
```

## Residual Diagnostic
### Standardized residual ACF

```{r}
# Standardized residuals
plot(sGARCH, which = 10)
```

## Residual Diagnostic
### Standardaied Squared residuals ACF
```{r}
# Standardaied Squared residuals
plot(sGARCH, which = 11)
```

## Forecast for fitted model
```{r}
forec <- ugarchforecast(sGARCH, data, n.ahead = 5)
forec
```

It's convenient to use ugarchforecast() for forecast future returns, but it will have look-ahead bias, which it use the information that is not yet available or known. So we use...

## Rolling Forecast for window size 1000

```{r echo = TRUE}
roll <- ugarchroll(spec = spec, 
                   data = data,
                   n.ahead = 1,
                   n.start = 3000,
                   refit.every = 50,
                   refit.window = "moving",
                   solver = "hybrid",
                   window.size = 1000,
                   keep.coef = TRUE)
```

Refit in moving window where all previous data is used for the first estimation and then moved by a length equal to refit.every

## Rolling Forecast for window size 1000

```{r}
show(roll)
```

## Rolling Forecast for window size 1000

### refit.window

![](images/moving_window.png)

Refit in moving window where all previous data is used for the first estimation and then moved by a length equal to refit.every.
Another refit window is "recursive", which expand the window size including all the previous data.


## Error Metircs

```{r echo = TRUE}
rugarch::report(roll, type = "fpm")
```



# RNN-LSTM Model

## Recurrent Neural Network(RNN)

A **recurrent neural network** is a class of artificial neural network that uses sequential or time series data. Unlike Feedforward Neural Network, RNN allows the output from some nodes to affect subsequent input to the same nodes by using connections between nodes to create cycles. As a result, the hidden layers produce the outputs with the input information and prior "memory" received from previous learning.

## Unroll RNN

![Rolled RNN and Unrolled RNN](images/unroll-RNN.png)

## Recurrent Neural Network(RNN)

Another distinguish characteristic of RNN is that they share parameters across each layer of the network. Unlike feedforward neural networks having individual weight and bias for each node in one layer, recurrent neural networks share the same weight parameter within each layer. However, these weights are still adjusted during the processes of backpropagation and gradient descent to facilitate reinforcement learning.

In feedforward neural network, backpropagation algorithm was used to calculate the gradient with respect to the weights. Recurrent neural network, on the other side, leverage backpropagation through time (BPTT) algorithm to determine the gradient as BPTT is specific to sequential data.

## Activation Functions

In neural networks, an activation function determines whether a neuron should be activated and typically maps the input to $[0, 1]$ or $[-1, 1]$. The followings are two of the most commonly used activation functions and will be adopted later:

#### Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

#### Tanh (Hyperbolic tangent)

$$
tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$

#### ReLU (Rectified Linear Unit) Activation Function

$$
R(x) = max(0, x)
$$

## Long Short-term Memory (LSTM)

Long short-term memory network, usually known as LSTM, is a specific RNN architecture first introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. Recall with an RNN, similar with human reading a book and remembering what happened in the earlier chapter, it remembers the previous information and use it for processing the current input. The shortcoming of the NN is that it is not able to remember long term dependencies due to the vanishing gradient. The LSTM is designed to alleviate and avoid such issues.

The LSTM consists of three parts:

-   **Forget Gate**: Choose whether the information coming from the previous time stamp should be remembered or can be forgotten
-   **Input Gate**: Learn new information from the input to this cell
-   **Output Gate**: Passes the updated information tot the next time stamp

## Forget Gate

In an LSTM cell, the cell first need to decide if the information from previous time stamp should be kept or forgotten. The equation of the forget gate is:

$$
f_t = \sigma(W_f\cdot[x_t, h_{t-1}] + b_f)
$$

Where

-   $x_t = \text{input to the current time stamp}$
-   $h_{t-1} = \text{hidden state of the previous time stamp}$
-   $W_f = \text{weight matrix associated with hidden state}$
-   $b_f = \text{constant}$

After that, a `sigmoid` function is applied over $f_t$ and make it a number between $0$ and $1$. Then $f_t$ is multiplied with the previous cell state. If $f_t=0$, the network will forget everything from the previous time stamp while $f_t=1$ represents that the network will remember everything.

## Input Gate and new information

Next we decide what new information we will store in the cell state. First, the input gate decides which values we'll update with `sigmoid` activation function:

$$
i_t = \sigma(W_i\cdot[x_t, h_{t-1}] + b_i)
$$

Where

-   $W_t = \text{weight matrix of the input associated with hidden state}$

Next, the new information is sent through a `tanh` layer to create the new candidate values:

$$
\tilde{C_t} = tanh(W_C\cdot[x_t, h_{t-1}] + b_C)
$$

## New Cell State $C_t$

With previous work, the LSTM cell now updates the new cell state for the current time stamp as:

$$
C_t = f_t * C_{t-1} + i_t * \tilde{C_t}
$$

The current cell state $C_t$ combines how much we decide to remember from the previous cell state $C_{t-1}$ scaled by the forget gate and how much we wish to take in from the new current input $\tilde{C_t}$ scaled by the input gate.

## Output Gate

Finally, the cell needs to decide what it is going to output and by how much. The filter of the output is the output gate, with the following equation:

$$
o_t = \sigma(W_o\cdot[x_t, h_{t-1}] + b_o)
$$

The equation of the output gate is very similar with the forget gate and the input gate. Then, we push the cell state $C_t$ through the `tanh` activation function to maintain the value staying in between $-1$ and $1$, and multiply it by the output gate:

$$
h_t = o_t * tanh(C_t)
$$

## Overall Module

The previous steps conclude the architecture of the LSTM. The whole process can be summarized and displayed as the following:

![LSTM Chain](images/LSTM-chain.png)

# RNN-LSTM Model - Coding

## Application in Tensorflow

We implement the LSTM by using the package `tensorflow`. The LSTM model in the `keras` package of `tensorflow` follows our previous description by using the `tanh` function as the activation function and the `ReLU` function as the recurrent activation function.

Let's start with Reproduce.

## Reproduce - Understand Article

![Design Overview of RNN Model](Images/RNN_reproduce_structure.png)

## Reproduce - Understand Article

### Data: Stock price of five companies from "2013-01-01" to "2017-12-31"

> "In particular, a total of 1259 days of data was collected. The first 1008 data points are for the first 4 years (2013 to 2016) and the last 251 data are for each day in 2017."

\vspace{0.5cm}

### Data Process: Scaling: Min-Max Scale

$$
x' = \frac{x - min(x)}{max(x) - min(x)}
$$ We will need to rescale data back after predicting.

## Reproduce - Understand Article

### Parameter

![](Images/RNN_parameter.png)

### LSTM Cell

![](Images/RNN_lstm_cell.png)

### Loss

![](Images/RNN_loss.png)

## Reproduce - Understand Article

![The Neural Network Loop](Images/RNN_loop.png)

## Reproduce - Data

```{r, echo=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyquant)
library(tidyverse)
library(tensorflow)
library(keras)
```

```{r, echo=TRUE}
start_date = "2013-01-01"; end_date = "2017-12-31"

nflx_prices <- tq_get("NFLX", get = "stock.prices",
                 from = start_date, to = end_date)

stock <- nflx_prices |> arrange(date) |>
  select(date, adjusted) |> column_to_rownames('date')

head(stock, 5)
```

## Reproduce - Train-Test Split and Scale

```{r, echo=TRUE}
# Max Min Scale
max_min_scale <- function(x, name = 'value') {
  df <- data.frame((x- min(x)) /(max(x)-min(x)))
  colnames(df) <- name; df}
max_min_scale_reverse <- function(y, x) {
  min(x) + y * (max(x)-min(x))}

train_set <- stock[1:1008,]
test_set <- stock[1009:nrow(stock),]
train_scaled <- train_set %>% max_min_scale
head(train_scaled, 3)
```

## Reproduce - Data Preperation for Fitting

```{r, echo=TRUE}
data_prep <- function(scaled_data, prediction = 1, lag = 22){
  x_data <- t(sapply(1:(dim(scaled_data)[1] - lag - prediction + 1),
                  function(x) scaled_data[x: (x + lag - 1), 1]))
  x_arr <- array(data = as.numeric(unlist(x_data)), 
                 dim = c(nrow(x_data), lag, 1))
  y_data <- t(sapply((1 + lag):(dim(scaled_data)[1] - prediction + 1), 
                     function(x) scaled_data[x: (x + prediction - 1), 1])) 
  y_arr <- array(data = as.numeric(unlist(y_data)), 
                 dim = c(length(y_data), prediction, 1))
  return(list(x = x_arr, y = y_arr))}

x_train = data_prep(train_scaled)$x
y_train = data_prep(train_scaled)$y
cat('x_dim: (', dim(x_train), ') || y_dim: (', dim(y_train), ')')
```

```{r}
cat('x dim:', dim(x_train))
```

## Reproduce - LSTM Setup

```{r, echo=TRUE}
num_neurons = 50
learning_rate = 0.002
# Define LSTM
get_model <- function(){
  model <- keras_model_sequential() |>
    layer_lstm(units = num_neurons, 
               batch_input_shape = c(1, 22, 1), 
               activation = 'relu', 
               stateful = TRUE) |>
    layer_dense(units = 1) 
  
  model %>% compile(loss = 'mse', metrics = 'mae',
    optimizer=optimizer_adam(learning_rate=learning_rate))
}
```

## Reproduce - LSTM Structure

```{r, echo=TRUE}
lstm_model <- get_model()
summary(lstm_model)
```

## Reproduce - LSTM Fitting

```{r, echo=TRUE}
set_random_seed(1209)
lstm_model <- get_model()
lstm_model %>% fit(x = x_train, y = y_train, batch_size = 1,
    epochs = 15, verbose = 2, shuffle = FALSE) -> history
```

```{r, fig.width=6, fig.height=2}
data.frame("x" = seq(1, 15), "y" = history$metrics$loss) -> df

df %>% ggplot(aes(x = x, y = y)) +
    geom_line(col = '#42a5f5') +
  geom_point(col = '#2c8fdf') + 
  labs(title = 'Loss History',
       x = 'Epoch', y = 'Loss(mse)')
```

## Reproduce - LSTM Result

```{r, echo=TRUE}
model_prediction_lstm <- function(test_len, whole_data, model){
  t <- test_len # test size
  Tt <- nrow(whole_data) # whole data size
  t_start <- Tt - t + 1 # test start date
  predictions <- vector(length = t)
  for (i in 1:t){
    n <- t_start - 1 + i
    test_set = pull(whole_data)[(n-22):n]
    test_scaled <- test_set %>% max_min_scale
    x_test = data_prep(test_scaled)$x
    y_pred_scaled <- stats::predict(model, x_test, verbose = 0)
    y_pred <- max_min_scale_reverse(y_pred_scaled, test_set)
    predictions[i] <- y_pred
  }
  pred_df <- data.frame('date' = rownames(whole_data)[t_start:Tt] %>% as_date(),
    'pred' = predictions, 'actual' = pull(whole_data)[t_start:Tt])
  pred_df
}
```

## Reproduce - LSTM Result

```{r, echo=TRUE}
model_prediction_lstm(length(test_set), stock, lstm_model) -> pred_df
pred_df %>% head(2)
```

```{r, fig.width=4, fig.height=2}
pred_df_long <- pred_df %>% pivot_longer(cols = c(pred, actual), 
                       names_to = "type", values_to = "price")

pred_df_long %>% ggplot(aes(x = date, y = price, color = type)) +
  geom_line() +
  labs(x = 'Date', y = 'Price($)', color = 'Type') +
  theme(legend.position = 'top')
```

## Result Comparison - Article Result

![Comparison of NFLX Monthly Mean](images/RNN_monthly_result.png)

## Result Comparison - Our Result

```{r, fig.width=5, fig.height=3.5}
pred_df_monthly <- pred_df_long %>% mutate(month = month.abb[month(date)]) %>%
  group_by(month, type) %>% summarise(avg_price = mean(price), .groups = 'drop')

pred_df_monthly$month <- factor(pred_df_monthly$month, levels = month.abb)

ggplot(pred_df_monthly, aes(x=month, y=avg_price, fill=type)) +
  geom_bar(stat = "identity", position="dodge", width = 0.65) +
  scale_fill_manual(values = c("actual" = '#393b79', 'pred' = '#de9ed6')) +
  labs(title = 'Comparison of NFLX Monthly Mean, 2017',
       y = 'Adjusted Price', x = 'Date', fill = '') +
  theme_classic() +
  theme(legend.position = 'bottom') 
```

## Result Comparison - \|% of Error\|

```{r}
data.frame('mean'= 2.00, 'std'= 1.54, 'variabce' = 2.38,
           'min' = 0.00, '25_percentile' = 0.80, '50_percentile' = 1.65, 
           '75_percentile' = 2.96, 'max' = 9.25, 'R_squared' = 0.9589) %>% 
  pivot_longer(cols = c(mean:R_squared), names_to = '|% of Error|', 
               values_to = 'Article') -> comparison

pred = pred_df$pred
y = pred_df$actual
rsq <- sum((pred_df$pred-mean(y))^2)/sum((y-mean(y))^2)

per_err <- pred_df %>% mutate(percent_err = abs(pred-actual)/actual * 100) %>% 
  pull(percent_err)
```

```{r,echo=TRUE}
comparison %>% cbind('Our Result' = c(mean(per_err), StdDev(per_err),
                      StdDev(per_err)^2, quantile(per_err), rsq))
```

## New Model - Get S&P 500, Set val

```{r, echo=TRUE}
sp_prices <- tq_get("^GSPC", get = "stock.prices",
                 from = "2000-01-01", to = "2021-12-31")
return <- sp_prices |> arrange(date) |>
  mutate(return = log(adjusted) - log(lag(adjusted))) |>
  drop_na(return) %>% select(date, return)
# scale
return$return <- max_min_scale(return$return) %>% pull(value)
# train test split
val_date <- "2019-01-01"; test_date <- '2020-01-01'
train_data_n <- return |> filter(date <= val_date) %>% column_to_rownames('date')
val_data_n <- return |> filter(date < test_date & date > val_date) %>% column_to_rownames('date')
test_data_n <- return |> filter(date >= test_date) %>% column_to_rownames('date')
# data preparation
x_train_new = data_prep(train_data_n)$x; y_train_new = data_prep(train_data_n)$y
x_val_new = data_prep(val_data_n)$x; y_val_new = data_prep(val_data_n)$y
```

```{r}
cat('x_tr_dim: (', dim(x_train_new), ') || y_tr_dim: (', dim(y_train_new), ')\n')
cat('x_val_dim: (', dim(x_val_new), ') || y_val_dim: (', dim(y_val_new), ')')
```

## New Model - Fit LSTM

```{r}
return_new <- return %>% column_to_rownames('date')

model_eval <- function(y, pred){
  mse <- mean((pred - y)^2)
  mae <- mean(abs(pred-y))
  rmse <- sqrt(mean(pred-y)^2)
  rsq <- sum((pred-mean(y))^2)/sum((y-mean(y))^2)
  data.frame('MSE' = mse, 'MAE' = mae, 'RMSE' = rmse, 'Rsq' = rsq)
}
```

```{r, echo=TRUE}
validation_data <- list(x_val_new, y_val_new)
set_random_seed(1209)
lstm_model2 <- get_model()
lstm_model2 %>% fit(x = x_train_new, y = y_train_new, batch_size = 1,
    epochs = 5, verbose = 2, shuffle = FALSE, 
    validation_data = validation_data) -> history2

pred_df2 <- model_prediction_lstm(nrow(test_data_n), return_new, lstm_model2)
head(pred_df2, 3)
```

## New Model - Loss History

```{r}
plot(history2)
```

## New Model - Prediction

```{r}
model_eval(pred_df2$actual, pred_df2$pred) -> eval_lstm
eval_lstm
```

```{r, fig.width=4, fig.height=2}
pred_df2_long <- pred_df2 %>% pivot_longer(cols = c(pred, actual), 
                       names_to = "type", values_to = "price")

pred_df2_long %>% ggplot(aes(x = date, y = price, color = type)) +
  geom_line() +
  labs(x = 'Date', y = 'Log Return', color = 'Type') +
  theme(legend.position = 'top')
```

# Model Comparison - ARIMA and LSTM

## Error Metrics

```{r}
data.frame('MSE'= 0.0001065, 'MAE'= 0.0065320) %>% 
  pivot_longer(cols = c(MSE:MAE), names_to = 'Error Metrics', 
               values_to = 'sGARCH') -> comparison2

comparison2 %>% cbind('RNN_LSTM' = c(eval_lstm$MSE, eval_lstm$MAE))
```


## Thank Your

Any Questions?

